{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# map_reduce_and_visualization.py - ELI5 Walkthrough\n",
    "This notebook unpacks `python/samples/getting_started/workflows/parallelism/map_reduce_and_visualization.py` into guided sections with a workflow diagram.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Big Picture\n",
    "We split a long document into chunks, map over the tokens, shuffle intermediate results to reducers, and finally aggregate word counts. Along the way we persist intermediate files and show how to visualize the workflow graph.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Ingredients\n",
    "- Fan-out/fan-in edges express the classic map-reduce stages.\n",
    "- Shared state lets each mapper read its token range without copying large payloads.\n",
    "- `WorkflowViz` can emit Mermaid/DiGraph strings or export an SVG when the viz extra is installed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Workflow Diagram\n",
    "```mermaid\n",
    "flowchart LR\n",
    "    Start([\"Raw Text\"]) --> Split[[Split]]\n",
    "    Split --> Map1[[Map 1]]\n",
    "    Split --> Map2[[Map 2]]\n",
    "    Split --> Map3[[Map 3]]\n",
    "    Map1 --> Shuffle[[Shuffle]]\n",
    "    Map2 --> Shuffle\n",
    "    Map3 --> Shuffle\n",
    "    Shuffle --> Reduce1[[Reduce 1]]\n",
    "    Shuffle --> Reduce2[[Reduce 2]]\n",
    "    Shuffle --> Reduce3[[Reduce 3]]\n",
    "    Shuffle --> Reduce4[[Reduce 4]]\n",
    "    Reduce1 --> Complete[[CompletionExecutor]]\n",
    "    Reduce2 --> Complete\n",
    "    Reduce3 --> Complete\n",
    "    Reduce4 --> Complete\n",
    "    Complete --> Output([\"Reducer Files\"])\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Imports, constants, and scenario overview\n",
    "We load workflow utilities, configure a temp directory, and keep the docstring that explains the map-reduce stages.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) Microsoft. All rights reserved.\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "import ast\n",
    "import asyncio\n",
    "import os\n",
    "from collections import defaultdict\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import aiofiles\n",
    "from agent_framework import (\n",
    "    Executor,  # Base class for custom workflow steps\n",
    "    WorkflowBuilder,  # Fluent builder for executors and edges\n",
    "    WorkflowContext,  # Per run context with shared state and messaging\n",
    "    WorkflowOutputEvent,  # Event emitted when workflow yields output\n",
    "    WorkflowViz,  # Utility to visualize a workflow graph\n",
    "    handler,  # Decorator to expose an Executor method as a step\n",
    ")\n",
    "from typing_extensions import Never\n",
    "\n",
    "\"\"\"\n",
    "Sample: Map reduce word count with fan out and fan in over file backed intermediate results\n",
    "\n",
    "The workflow splits a large text into chunks, maps words to counts in parallel,\n",
    "shuffles intermediate pairs to reducers, then reduces to per word totals.\n",
    "It also demonstrates WorkflowViz for graph visualization.\n",
    "\n",
    "Purpose:\n",
    "Show how to:\n",
    "- Partition input once and coordinate parallel mappers with shared state.\n",
    "- Implement map, shuffle, and reduce executors that pass file paths instead of large payloads.\n",
    "- Use fan out and fan in edges to express parallelism and joins.\n",
    "- Persist intermediate results to disk to bound memory usage for large inputs.\n",
    "- Visualize the workflow graph using WorkflowViz and export to SVG with the optional viz extra.\n",
    "\n",
    "Prerequisites:\n",
    "- Familiarity with WorkflowBuilder, executors, fan out and fan in edges, events, and streaming runs.\n",
    "- aiofiles installed for async file I/O.\n",
    "- Write access to a tmp directory next to this script.\n",
    "- A source text at resources/long_text.txt.\n",
    "- Optional for SVG export: install the viz extra for agent framework workflow.\n",
    "\"\"\"\n",
    "\n",
    "# Define the temporary directory for storing intermediate results\n",
    "DIR = os.path.dirname(__file__)\n",
    "TEMP_DIR = os.path.join(DIR, \"tmp\")\n",
    "# Ensure the temporary directory exists\n",
    "os.makedirs(TEMP_DIR, exist_ok=True)\n",
    "\n",
    "# Define a key for the shared state to store the data to be processed\n",
    "SHARED_STATE_DATA_KEY = \"data_to_be_processed\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split\n",
    "`Split` tokenizes the input, stores tokens in shared state, and assigns index ranges to each mapper.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Split(Executor):\n",
    "    \"\"\"Splits data into roughly equal chunks based on the number of mapper nodes.\"\"\"\n",
    "\n",
    "    def __init__(self, map_executor_ids: list[str], id: str | None = None):\n",
    "        \"\"\"Store mapper ids so we can assign non overlapping ranges per mapper.\"\"\"\n",
    "        super().__init__(id=id or \"split\")\n",
    "        self._map_executor_ids = map_executor_ids\n",
    "\n",
    "    @handler\n",
    "    async def split(self, data: str, ctx: WorkflowContext[SplitCompleted]) -> None:\n",
    "        \"\"\"Tokenize input and assign contiguous index ranges to each mapper via shared state.\n",
    "\n",
    "        Args:\n",
    "            data: The raw text to process.\n",
    "            ctx: Workflow context to persist shared state and send messages.\n",
    "        \"\"\"\n",
    "        # Process data into a list of words and remove empty lines or words.\n",
    "        word_list = self._preprocess(data)\n",
    "\n",
    "        # Store tokenized words once so all mappers can read by index.\n",
    "        await ctx.set_shared_state(SHARED_STATE_DATA_KEY, word_list)\n",
    "\n",
    "        # Divide indices into contiguous slices for each mapper.\n",
    "        map_executor_count = len(self._map_executor_ids)\n",
    "        chunk_size = len(word_list) // map_executor_count  # Assumes count > 0.\n",
    "\n",
    "        async def _process_chunk(i: int) -> None:\n",
    "            \"\"\"Assign the slice for mapper i, then signal that splitting is done.\"\"\"\n",
    "            start_index = i * chunk_size\n",
    "            end_index = start_index + chunk_size if i < map_executor_count - 1 else len(word_list)\n",
    "\n",
    "            # The mapper reads its slice from shared state keyed by its own executor id.\n",
    "            await ctx.set_shared_state(self._map_executor_ids[i], (start_index, end_index))\n",
    "            await ctx.send_message(SplitCompleted(), self._map_executor_ids[i])\n",
    "\n",
    "        tasks = [asyncio.create_task(_process_chunk(i)) for i in range(map_executor_count)]\n",
    "        await asyncio.gather(*tasks)\n",
    "\n",
    "    def _preprocess(self, data: str) -> list[str]:\n",
    "        \"\"\"Normalize lines and split on whitespace. Return a flat list of tokens.\"\"\"\n",
    "        line_list = [line.strip() for line in data.splitlines() if line.strip()]\n",
    "        return [word for line in line_list for word in line.split() if word]\n",
    "\n",
    "\n",
    "@dataclass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map\n",
    "Each `Map` executor reads its assigned slice from shared state and writes `(word, 1)` pairs to disk.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Map(Executor):\n",
    "    \"\"\"Maps each token to a count of 1 and writes pairs to a per mapper file.\"\"\"\n",
    "\n",
    "    @handler\n",
    "    async def map(self, _: SplitCompleted, ctx: WorkflowContext[MapCompleted]) -> None:\n",
    "        \"\"\"Read the assigned slice, emit (word, 1) pairs, and persist to disk.\n",
    "\n",
    "        Args:\n",
    "            _: SplitCompleted marker indicating maps can begin.\n",
    "            ctx: Workflow context for shared state access and messaging.\n",
    "        \"\"\"\n",
    "        # Retrieve tokens and our assigned slice.\n",
    "        data_to_be_processed: list[str] = await ctx.get_shared_state(SHARED_STATE_DATA_KEY)\n",
    "        chunk_start, chunk_end = await ctx.get_shared_state(self.id)\n",
    "\n",
    "        results = [(item, 1) for item in data_to_be_processed[chunk_start:chunk_end]]\n",
    "\n",
    "        # Write this mapper's results as simple text lines for easy debugging.\n",
    "        file_path = os.path.join(TEMP_DIR, f\"map_results_{self.id}.txt\")\n",
    "        async with aiofiles.open(file_path, \"w\") as f:\n",
    "            await f.writelines([f\"{item}: {count}\\n\" for item, count in results])\n",
    "\n",
    "        await ctx.send_message(MapCompleted(file_path))\n",
    "\n",
    "\n",
    "@dataclass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shuffle\n",
    "`Shuffle` groups map outputs by key, partitions them per reducer, and signals which reducer should handle each file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Shuffle(Executor):\n",
    "    \"\"\"Groups intermediate pairs by key and partitions them across reducers.\"\"\"\n",
    "\n",
    "    def __init__(self, reducer_ids: list[str], id: str | None = None):\n",
    "        \"\"\"Remember reducer ids so we can partition work deterministically.\"\"\"\n",
    "        super().__init__(id=id or \"shuffle\")\n",
    "        self._reducer_ids = reducer_ids\n",
    "\n",
    "    @handler\n",
    "    async def shuffle(self, data: list[MapCompleted], ctx: WorkflowContext[ShuffleCompleted]) -> None:\n",
    "        \"\"\"Aggregate mapper outputs and write one partition file per reducer.\n",
    "\n",
    "        Args:\n",
    "            data: MapCompleted records with file paths for each mapper output.\n",
    "            ctx: Workflow context to emit per reducer ShuffleCompleted messages.\n",
    "        \"\"\"\n",
    "        chunks = await self._preprocess(data)\n",
    "\n",
    "        async def _process_chunk(chunk: list[tuple[str, list[int]]], index: int) -> None:\n",
    "            \"\"\"Write one grouped partition for reducer index and notify that reducer.\"\"\"\n",
    "            file_path = os.path.join(TEMP_DIR, f\"shuffle_results_{index}.txt\")\n",
    "            async with aiofiles.open(file_path, \"w\") as f:\n",
    "                await f.writelines([f\"{key}: {value}\\n\" for key, value in chunk])\n",
    "            await ctx.send_message(ShuffleCompleted(file_path, self._reducer_ids[index]))\n",
    "\n",
    "        tasks = [asyncio.create_task(_process_chunk(chunk, i)) for i, chunk in enumerate(chunks)]\n",
    "        await asyncio.gather(*tasks)\n",
    "\n",
    "    async def _preprocess(self, data: list[MapCompleted]) -> list[list[tuple[str, list[int]]]]:\n",
    "        \"\"\"Load all mapper files, group by key, sort keys, and partition for reducers.\n",
    "\n",
    "        Returns:\n",
    "            List of partitions. Each partition is a list of (key, [1, 1, ...]) tuples.\n",
    "        \"\"\"\n",
    "        # Load all intermediate pairs.\n",
    "        map_results: list[tuple[str, int]] = []\n",
    "        for result in data:\n",
    "            async with aiofiles.open(result.file_path, \"r\") as f:\n",
    "                map_results.extend([\n",
    "                    (line.strip().split(\": \")[0], int(line.strip().split(\": \")[1])) for line in await f.readlines()\n",
    "                ])\n",
    "\n",
    "        # Group values by token.\n",
    "        intermediate_results: defaultdict[str, list[int]] = defaultdict(list[int])\n",
    "        for key, value in map_results:\n",
    "            intermediate_results[key].append(value)\n",
    "\n",
    "        # Deterministic ordering helps with debugging and test stability.\n",
    "        aggregated_results = [(key, values) for key, values in intermediate_results.items()]\n",
    "        aggregated_results.sort(key=lambda x: x[0])\n",
    "\n",
    "        # Partition keys across reducers as evenly as possible.\n",
    "        reduce_executor_count = len(self._reducer_ids)\n",
    "        chunk_size = len(aggregated_results) // reduce_executor_count\n",
    "        remaining = len(aggregated_results) % reduce_executor_count\n",
    "\n",
    "        chunks = [\n",
    "            aggregated_results[i : i + chunk_size] for i in range(0, len(aggregated_results) - remaining, chunk_size)\n",
    "        ]\n",
    "        if remaining > 0:\n",
    "            chunks[-1].extend(aggregated_results[-remaining:])\n",
    "\n",
    "        return chunks\n",
    "\n",
    "\n",
    "@dataclass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reduce\n",
    "Each `Reduce` executor sums counts for its partition and writes final word totals.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Reduce(Executor):\n",
    "    \"\"\"Sums grouped counts per key for its assigned partition.\"\"\"\n",
    "\n",
    "    @handler\n",
    "    async def _execute(self, data: ShuffleCompleted, ctx: WorkflowContext[ReduceCompleted]) -> None:\n",
    "        \"\"\"Read one shuffle partition and reduce it to totals.\n",
    "\n",
    "        Args:\n",
    "            data: ShuffleCompleted with the partition file path and target reducer id.\n",
    "            ctx: Workflow context used to emit ReduceCompleted with our output file path.\n",
    "        \"\"\"\n",
    "        if data.reducer_id != self.id:\n",
    "            # This partition belongs to a different reducer. Skip.\n",
    "            return\n",
    "\n",
    "        # Read grouped values from the shuffle output.\n",
    "        async with aiofiles.open(data.file_path, \"r\") as f:\n",
    "            lines = await f.readlines()\n",
    "\n",
    "        # Sum values per key. Values are serialized Python lists like [1, 1, ...].\n",
    "        reduced_results: dict[str, int] = defaultdict(int)\n",
    "        for line in lines:\n",
    "            key, value = line.split(\": \")\n",
    "            reduced_results[key] = sum(ast.literal_eval(value))\n",
    "\n",
    "        # Persist our partition totals.\n",
    "        file_path = os.path.join(TEMP_DIR, f\"reduced_results_{self.id}.txt\")\n",
    "        async with aiofiles.open(file_path, \"w\") as f:\n",
    "            await f.writelines([f\"{key}: {value}\\n\" for key, value in reduced_results.items()])\n",
    "\n",
    "        await ctx.send_message(ReduceCompleted(file_path))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CompletionExecutor\n",
    "The completion step collects reducer file paths and yields them as the workflow output.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CompletionExecutor(Executor):\n",
    "    \"\"\"Joins all reducer outputs and yields the final output.\"\"\"\n",
    "\n",
    "    @handler\n",
    "    async def complete(self, data: list[ReduceCompleted], ctx: WorkflowContext[Never, list[str]]) -> None:\n",
    "        \"\"\"Collect reducer output file paths and yield final output.\"\"\"\n",
    "        await ctx.yield_output([result.file_path for result in data])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Run the workflow and visualize it\n",
    "`main()` wires all stages together, prints the Mermaid/DiGraph output from `WorkflowViz`, and runs the word-count demo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def main():\n",
    "    \"\"\"Construct the map reduce workflow, visualize it, then run it over a sample file.\"\"\"\n",
    "    # Step 1: Create the executors.\n",
    "    map_operations = [Map(id=f\"map_executor_{i}\") for i in range(3)]\n",
    "    split_operation = Split(\n",
    "        [map_operation.id for map_operation in map_operations],\n",
    "        id=\"split_data_executor\",\n",
    "    )\n",
    "    reduce_operations = [Reduce(id=f\"reduce_executor_{i}\") for i in range(4)]\n",
    "    shuffle_operation = Shuffle(\n",
    "        [reduce_operation.id for reduce_operation in reduce_operations],\n",
    "        id=\"shuffle_executor\",\n",
    "    )\n",
    "    completion_executor = CompletionExecutor(id=\"completion_executor\")\n",
    "\n",
    "    # Step 2: Build the workflow graph using fan out and fan in edges.\n",
    "    workflow = (\n",
    "        WorkflowBuilder()\n",
    "        .set_start_executor(split_operation)\n",
    "        .add_fan_out_edges(split_operation, map_operations)  # Split -> many mappers\n",
    "        .add_fan_in_edges(map_operations, shuffle_operation)  # All mappers -> shuffle\n",
    "        .add_fan_out_edges(shuffle_operation, reduce_operations)  # Shuffle -> many reducers\n",
    "        .add_fan_in_edges(reduce_operations, completion_executor)  # All reducers -> completion\n",
    "        .build()\n",
    "    )\n",
    "\n",
    "    # Step 2.5: Visualize the workflow (optional)\n",
    "    print(\"Generating workflow visualization...\")\n",
    "    viz = WorkflowViz(workflow)\n",
    "    # Print out the Mermaid string.\n",
    "    print(\"Mermaid string: \\n=======\")\n",
    "    print(viz.to_mermaid())\n",
    "    print(\"=======\")\n",
    "    # Print out the DiGraph string.\n",
    "    print(\"DiGraph string: \\n=======\")\n",
    "    print(viz.to_digraph())\n",
    "    print(\"=======\")\n",
    "    try:\n",
    "        # Export the DiGraph visualization as SVG.\n",
    "        svg_file = viz.export(format=\"svg\")\n",
    "        print(f\"SVG file saved to: {svg_file}\")\n",
    "    except ImportError:\n",
    "        print(\"Tip: Install 'viz' extra to export workflow visualization: pip install agent-framework[viz] --pre\")\n",
    "\n",
    "    # Step 3: Open the text file and read its content.\n",
    "    async with aiofiles.open(os.path.join(DIR, \"../resources\", \"long_text.txt\"), \"r\") as f:\n",
    "        raw_text = await f.read()\n",
    "\n",
    "    # Step 4: Run the workflow with the raw text as input.\n",
    "    async for event in workflow.run_stream(raw_text):\n",
    "        print(f\"Event: {event}\")\n",
    "        if isinstance(event, WorkflowOutputEvent):\n",
    "            print(f\"Final Output: {event.data}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try it yourself\n",
    "Use the helper below. In notebooks it awaits `main()` on the active loop; in scripts it falls back to `asyncio.run(main())`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "\n",
    "# Helper for notebooks vs. scripts\n",
    "loop = asyncio.get_event_loop()\n",
    "if loop.is_running():\n",
    "    # Jupyter/VS Code notebooks already have an event loop, so await directly.\n",
    "    await main()\n",
    "else:\n",
    "    asyncio.run(main())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
