{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# guessing_game_with_human_input.py - ELI5 Walkthrough\n",
    "This notebook recreates `python/samples/getting_started/workflows/human-in-the-loop/guessing_game_with_human_input.py` so you can learn it step by step.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Big Picture\n",
    "An Azure AI agent plays a number guessing game. After each guess the workflow pauses for a human to say higher, lower, or correct. Once the human confirms the answer, the workflow finishes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Ingredients\n",
    "- `RequestInfoExecutor` pauses execution to collect human input and resumes when you supply a response.\n",
    "- `TurnManager` orchestrates the back-and-forth among agent, human, and workflow.\n",
    "- `AgentExecutor` runs an Azure-hosted agent that always returns JSON (`{\"guess\": <int>}`) using `response_format`.\n",
    "- Streaming APIs (`run_stream`, `send_responses_streaming`) keep the workflow responsive while you type answers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b898938d",
   "metadata": {},
   "source": [
    "### Workflow Diagram\n",
    "```mermaid\n",
    "flowchart LR\n",
    "    Start([\"Start Game\"]) --> Turn[[TurnManager]]\n",
    "    Turn --> Agent[[AgentExecutor]]\n",
    "    Agent --> Turn\n",
    "    Turn --> Request[/RequestInfoExecutor/]\n",
    "    Request --> Human[\"Human Feedback\"]\n",
    "    Human --> Turn\n",
    "    Turn --> Output([\"Guessed Correctly\"])\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Load dependencies and explain the sample\n",
    "Here we pull in the Agent Framework pieces, set up environment loading, and include a detailed module docstring describing the game.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc97c0b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nSample: Human in the loop guessing game\\n\\nAn agent guesses a number, then a human guides it with higher, lower, or\\ncorrect via RequestInfoExecutor. The loop continues until the human confirms\\ncorrect, at which point the workflow completes when idle with no pending work.\\n\\nPurpose:\\nShow how to integrate a human step in the middle of an LLM workflow using RequestInfoExecutor and correlated\\nRequestResponse objects.\\n\\nDemonstrate:\\n- Alternating turns between an AgentExecutor and a human, driven by events.\\n- Using Pydantic response_format to enforce structured JSON output from the agent instead of regex parsing.\\n- Driving the loop in application code with run_stream and send_responses_streaming.\\n\\nPrerequisites:\\n- Azure OpenAI configured for AzureOpenAIChatClient with required environment variables.\\n- Authentication via azure-identity. Use AzureCliCredential and run az login before executing the sample.\\n- Basic familiarity with WorkflowBuilder, executors, edges, events, and streaming runs.\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Copyright (c) Microsoft. All rights reserved.\n",
    "\n",
    "import asyncio\n",
    "from dataclasses import dataclass\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "from agent_framework import (\n",
    "    AgentExecutor,  # Executor that runs the agent\n",
    "    AgentExecutorRequest,  # Message bundle sent to an AgentExecutor\n",
    "    AgentExecutorResponse,  # Result returned by an AgentExecutor\n",
    "    ChatMessage,  # Chat message structure\n",
    "    Executor,  # Base class for workflow executors\n",
    "    RequestInfoEvent,  # Event emitted when human input is requested\n",
    "    RequestInfoExecutor,  # Special executor that collects human input out of band\n",
    "    RequestInfoMessage,  # Base class for request payloads sent to RequestInfoExecutor\n",
    "    RequestResponse,  # Correlates a human response with the original request\n",
    "    Role,  # Enum of chat roles (user, assistant, system)\n",
    "    WorkflowBuilder,  # Fluent builder for assembling the graph\n",
    "    WorkflowContext,  # Per run context and event bus\n",
    "    WorkflowOutputEvent,  # Event emitted when workflow yields output\n",
    "    WorkflowRunState,  # Enum of workflow run states\n",
    "    WorkflowStatusEvent,  # Event emitted on run state changes\n",
    "    handler,  # Decorator to expose an Executor method as a step\n",
    ")\n",
    "from agent_framework.azure import AzureOpenAIChatClient\n",
    "from azure.identity import AzureCliCredential\n",
    "from pydantic import BaseModel\n",
    "\n",
    "\"\"\"\n",
    "Sample: Human in the loop guessing game\n",
    "\n",
    "An agent guesses a number, then a human guides it with higher, lower, or\n",
    "correct via RequestInfoExecutor. The loop continues until the human confirms\n",
    "correct, at which point the workflow completes when idle with no pending work.\n",
    "\n",
    "Purpose:\n",
    "Show how to integrate a human step in the middle of an LLM workflow using RequestInfoExecutor and correlated\n",
    "RequestResponse objects.\n",
    "\n",
    "Demonstrate:\n",
    "- Alternating turns between an AgentExecutor and a human, driven by events.\n",
    "- Using Pydantic response_format to enforce structured JSON output from the agent instead of regex parsing.\n",
    "- Driving the loop in application code with run_stream and send_responses_streaming.\n",
    "\n",
    "Prerequisites:\n",
    "- Azure OpenAI configured for AzureOpenAIChatClient with required environment variables.\n",
    "- Authentication via azure-identity. Use AzureCliCredential and run az login before executing the sample.\n",
    "- Basic familiarity with WorkflowBuilder, executors, edges, events, and streaming runs.\n",
    "\"\"\"\n",
    "\n",
    "# What RequestInfoExecutor does:\n",
    "# RequestInfoExecutor is a workflow-native bridge that pauses the graph at a request for information,\n",
    "# emits a RequestInfoEvent with a typed payload, and then resumes the graph only after your application\n",
    "# supplies a matching RequestResponse keyed by the emitted request_id. It does not gather input by itself.\n",
    "# Your application is responsible for collecting the human reply from any UI or CLI and then calling\n",
    "# send_responses_streaming with a dict mapping request_id to the human's answer. The executor exists to\n",
    "# standardize pause-and-resume human gating, to carry typed request payloads, and to preserve correlation.\n",
    "\n",
    "\n",
    "# Request type sent to the RequestInfoExecutor for human feedback.\n",
    "# Including the agent's last guess allows the UI or CLI to display context and helps\n",
    "# the turn manager avoid extra state reads.\n",
    "# Why subclass RequestInfoMessage:\n",
    "# Subclassing RequestInfoMessage defines the exact schema of the request that the human will see.\n",
    "# This gives you strong typing, forward-compatible validation, and clear correlation semantics.\n",
    "# It also lets you attach contextual fields (such as the previous guess) so the UI can render a rich prompt\n",
    "# without fetching extra state from elsewhere.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f0d32b0",
   "metadata": {},
   "source": [
    "### Step 2: Define the data contracts\n",
    "`HumanFeedbackRequest` models the prompt sent to the human, and `GuessOutput` enforces the agent's JSON reply. Subclassing `RequestInfoMessage` keeps checkpointing and correlation simple.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1c0fb745",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class HumanFeedbackRequest(RequestInfoMessage):\n",
    "    prompt: str = \"\"\n",
    "    guess: int | None = None\n",
    "\n",
    "\n",
    "class GuessOutput(BaseModel):\n",
    "    \"\"\"Structured output from the agent. Enforced via response_format for reliable parsing.\"\"\"\n",
    "\n",
    "    guess: int\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1b3f0c1",
   "metadata": {},
   "source": [
    "### Step 3: Coordinate turns between agent and human\n",
    "`TurnManager` starts the game, parses structured guesses, asks the human for feedback, and either finishes the game or loops back to the agent.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "060c3041",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TurnManager(Executor):\n",
    "    \"\"\"Coordinates turns between the agent and the human.\n",
    "\n",
    "    Responsibilities:\n",
    "    - Kick off the first agent turn.\n",
    "    - After each agent reply, request human feedback with a HumanFeedbackRequest.\n",
    "    - After each human reply, either finish the game or prompt the agent again with feedback.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, id: str | None = None):\n",
    "        super().__init__(id=id or \"turn_manager\")\n",
    "\n",
    "    @handler\n",
    "    async def start(self, _: str, ctx: WorkflowContext[AgentExecutorRequest]) -> None:\n",
    "        \"\"\"Start the game by asking the agent for an initial guess.\n",
    "\n",
    "        Contract:\n",
    "        - Input is a simple starter token (ignored here).\n",
    "        - Output is an AgentExecutorRequest that triggers the agent to produce a guess.\n",
    "        \"\"\"\n",
    "        user = ChatMessage(Role.USER, text=\"Start by making your first guess.\")\n",
    "        await ctx.send_message(AgentExecutorRequest(messages=[user], should_respond=True))\n",
    "\n",
    "    @handler\n",
    "    async def on_agent_response(\n",
    "        self,\n",
    "        result: AgentExecutorResponse,\n",
    "        ctx: WorkflowContext[HumanFeedbackRequest],\n",
    "    ) -> None:\n",
    "        \"\"\"Handle the agent's guess and request human guidance.\n",
    "\n",
    "        Steps:\n",
    "        1) Parse the agent's JSON into GuessOutput for robustness.\n",
    "        2) Send a HumanFeedbackRequest to the RequestInfoExecutor with a clear instruction:\n",
    "           - higher means the human's secret number is higher than the agent's guess.\n",
    "           - lower means the human's secret number is lower than the agent's guess.\n",
    "           - correct confirms the guess is exactly right.\n",
    "           - exit quits the demo.\n",
    "        \"\"\"\n",
    "        # Parse structured model output (defensive default if the agent did not reply).\n",
    "        text = result.agent_run_response.text or \"\"\n",
    "        last_guess = GuessOutput.model_validate_json(text).guess if text else None\n",
    "\n",
    "        # Craft a precise human prompt that defines higher and lower relative to the agent's guess.\n",
    "        prompt = (\n",
    "            f\"The agent guessed: {last_guess if last_guess is not None else text}. \"\n",
    "            \"Type one of: higher (your number is higher than this guess), \"\n",
    "            \"lower (your number is lower than this guess), correct, or exit.\"\n",
    "        )\n",
    "        await ctx.send_message(HumanFeedbackRequest(prompt=prompt, guess=last_guess))\n",
    "\n",
    "    @handler\n",
    "    async def on_human_feedback(\n",
    "        self,\n",
    "        feedback: RequestResponse[HumanFeedbackRequest, str],\n",
    "        ctx: WorkflowContext[AgentExecutorRequest, str],\n",
    "    ) -> None:\n",
    "        \"\"\"Continue the game or finish based on human feedback.\n",
    "\n",
    "        The RequestResponse contains both the human's string reply and the correlated HumanFeedbackRequest,\n",
    "        which carries the prior guess for convenience.\n",
    "        \"\"\"\n",
    "        reply = (feedback.data or \"\").strip().lower()\n",
    "        # Prefer the correlated request's guess to avoid extra shared state reads.\n",
    "        last_guess = getattr(feedback.original_request, \"guess\", None)\n",
    "\n",
    "        if reply == \"correct\":\n",
    "            await ctx.yield_output(f\"Guessed correctly: {last_guess}\")\n",
    "            return\n",
    "\n",
    "        # Provide feedback to the agent to try again.\n",
    "        # We keep the agent's output strictly JSON to ensure stable parsing on the next turn.\n",
    "        user_msg = ChatMessage(\n",
    "            Role.USER,\n",
    "            text=(f'Feedback: {reply}. Return ONLY a JSON object matching the schema {{\"guess\": <int 1..10>}}.'),\n",
    "        )\n",
    "        await ctx.send_message(AgentExecutorRequest(messages=[user_msg], should_respond=True))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c8d51a1",
   "metadata": {},
   "source": [
    "### Step 4: Build and run the interactive loop\n",
    "`main()` wires the workflow graph, streams events, prompts the human, and prints the final result. The loop alternates between running the workflow and feeding back human responses.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee212ab",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "asyncio.run() cannot be called from a running event loop",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 123\u001b[0m\n\u001b[0;32m    107\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    108\u001b[0m \u001b[38;5;124;03m    Sample Output:\u001b[39;00m\n\u001b[0;32m    109\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;124;03m    Workflow output: Guessed correctly: 9\u001b[39;00m\n\u001b[0;32m    119\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m  \u001b[38;5;66;03m# noqa: E501\u001b[39;00m\n\u001b[0;32m    122\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 123\u001b[0m     \u001b[43masyncio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\jacwang\\AppData\\Local\\anaconda3\\envs\\oai\\Lib\\asyncio\\runners.py:191\u001b[0m, in \u001b[0;36mrun\u001b[1;34m(main, debug, loop_factory)\u001b[0m\n\u001b[0;32m    161\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Execute the coroutine and return the result.\u001b[39;00m\n\u001b[0;32m    162\u001b[0m \n\u001b[0;32m    163\u001b[0m \u001b[38;5;124;03mThis function runs the passed coroutine, taking care of\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    187\u001b[0m \u001b[38;5;124;03m    asyncio.run(main())\u001b[39;00m\n\u001b[0;32m    188\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    189\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m events\u001b[38;5;241m.\u001b[39m_get_running_loop() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    190\u001b[0m     \u001b[38;5;66;03m# fail fast with short traceback\u001b[39;00m\n\u001b[1;32m--> 191\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    192\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124masyncio.run() cannot be called from a running event loop\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    194\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Runner(debug\u001b[38;5;241m=\u001b[39mdebug, loop_factory\u001b[38;5;241m=\u001b[39mloop_factory) \u001b[38;5;28;01mas\u001b[39;00m runner:\n\u001b[0;32m    195\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m runner\u001b[38;5;241m.\u001b[39mrun(main)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: asyncio.run() cannot be called from a running event loop"
     ]
    }
   ],
   "source": [
    "async def main() -> None:\n",
    "    # Create the chat agent and wrap it in an AgentExecutor.\n",
    "    # response_format enforces that the model produces JSON compatible with GuessOutput.\n",
    "    chat_client = AzureOpenAIChatClient(credential=AzureCliCredential())\n",
    "    agent = chat_client.create_agent(\n",
    "        instructions=(\n",
    "            \"You guess a number between 1 and 10. \"\n",
    "            \"If the user says 'higher' or 'lower', adjust your next guess. \"\n",
    "            'You MUST return ONLY a JSON object exactly matching this schema: {\"guess\": <integer 1..10>}. '\n",
    "            \"No explanations or additional text.\"\n",
    "        ),\n",
    "        response_format=GuessOutput,\n",
    "    )\n",
    "\n",
    "    # Build a simple loop: TurnManager <-> AgentExecutor <-> RequestInfoExecutor.\n",
    "    # TurnManager coordinates, AgentExecutor runs the model, RequestInfoExecutor gathers human replies.\n",
    "    turn_manager = TurnManager(id=\"turn_manager\")\n",
    "    agent_exec = AgentExecutor(agent=agent, id=\"agent\")\n",
    "\n",
    "    # Naming note:\n",
    "    # This variable is currently named hitl for historical reasons. The name can feel ambiguous or magical.\n",
    "    # Consider renaming to request_info_executor in your own code for clarity, since it directly represents\n",
    "    # the RequestInfoExecutor node that gathers human replies out of band.\n",
    "    hitl = RequestInfoExecutor(id=\"request_info\")\n",
    "\n",
    "    top_builder = (\n",
    "        WorkflowBuilder()\n",
    "        .set_start_executor(turn_manager)\n",
    "        .add_edge(turn_manager, agent_exec)  # Ask agent to make/adjust a guess\n",
    "        .add_edge(agent_exec, turn_manager)  # Agent's response comes back to coordinator\n",
    "        .add_edge(turn_manager, hitl)  # Ask human for guidance\n",
    "        .add_edge(hitl, turn_manager)  # Feed human guidance back to coordinator\n",
    "    )\n",
    "\n",
    "    # Build the workflow (no checkpointing in this minimal sample).\n",
    "    workflow = top_builder.build()\n",
    "\n",
    "    # Human in the loop run: alternate between invoking the workflow and supplying collected responses.\n",
    "    pending_responses: dict[str, str] | None = None\n",
    "    completed = False\n",
    "    workflow_output: str | None = None\n",
    "\n",
    "    # User guidance printing:\n",
    "    # If you want to instruct users up front, print a short banner before the loop.\n",
    "    # Example:\n",
    "    # print(\n",
    "    #     \"Interactive mode. When prompted, type one of: higher, lower, correct, or exit. \"\n",
    "    #     \"The agent will keep guessing until you reply correct.\",\n",
    "    #     flush=True,\n",
    "    # )\n",
    "\n",
    "    while not completed:\n",
    "        # First iteration uses run_stream(\"start\").\n",
    "        # Subsequent iterations use send_responses_streaming with pending_responses from the console.\n",
    "        stream = (\n",
    "            workflow.send_responses_streaming(pending_responses) if pending_responses else workflow.run_stream(\"start\")\n",
    "        )\n",
    "        # Collect events for this turn. Among these you may see WorkflowStatusEvent\n",
    "        # with state IDLE_WITH_PENDING_REQUESTS when the workflow pauses for\n",
    "        # human input, preceded by IN_PROGRESS_PENDING_REQUESTS as requests are\n",
    "        # emitted.\n",
    "        events = [event async for event in stream]\n",
    "        pending_responses = None\n",
    "\n",
    "        # Collect human requests, workflow outputs, and check for completion.\n",
    "        requests: list[tuple[str, str]] = []  # (request_id, prompt)\n",
    "        for event in events:\n",
    "            if isinstance(event, RequestInfoEvent) and isinstance(event.data, HumanFeedbackRequest):\n",
    "                # RequestInfoEvent for our HumanFeedbackRequest.\n",
    "                requests.append((event.request_id, event.data.prompt))\n",
    "            elif isinstance(event, WorkflowOutputEvent):\n",
    "                # Capture workflow output as they're yielded\n",
    "                workflow_output = str(event.data)\n",
    "                completed = True  # In this sample, we finish after one output.\n",
    "\n",
    "        # Detect run state transitions for a better developer experience.\n",
    "        pending_status = any(\n",
    "            isinstance(e, WorkflowStatusEvent) and e.state == WorkflowRunState.IN_PROGRESS_PENDING_REQUESTS\n",
    "            for e in events\n",
    "        )\n",
    "        idle_with_requests = any(\n",
    "            isinstance(e, WorkflowStatusEvent) and e.state == WorkflowRunState.IDLE_WITH_PENDING_REQUESTS\n",
    "            for e in events\n",
    "        )\n",
    "        if pending_status:\n",
    "            print(\"State: IN_PROGRESS_PENDING_REQUESTS (requests outstanding)\")\n",
    "        if idle_with_requests:\n",
    "            print(\"State: IDLE_WITH_PENDING_REQUESTS (awaiting human input)\")\n",
    "\n",
    "        # If we have any human requests, prompt the user and prepare responses.\n",
    "        if requests and not completed:\n",
    "            responses: dict[str, str] = {}\n",
    "            for req_id, prompt in requests:\n",
    "                # Simple console prompt for the sample.\n",
    "                print(f\"HITL> {prompt}\")\n",
    "                # Instructional print already appears above. The input line below is the user entry point.\n",
    "                # If desired, you can add more guidance here, but keep it concise.\n",
    "                answer = input(\"Enter higher/lower/correct/exit: \").lower()  # noqa: ASYNC250\n",
    "                if answer == \"exit\":\n",
    "                    print(\"Exiting...\")\n",
    "                    return\n",
    "                responses[req_id] = answer\n",
    "            pending_responses = responses\n",
    "\n",
    "    # Show final result from workflow output captured during streaming.\n",
    "    print(f\"Workflow output: {workflow_output}\")\n",
    "    \"\"\"\n",
    "    Sample Output:\n",
    "\n",
    "    HITL> The agent guessed: 5. Type one of: higher (your number is higher than this guess), lower (your number is lower than this guess), correct, or exit.\n",
    "    Enter higher/lower/correct/exit: higher\n",
    "    HITL> The agent guessed: 8. Type one of: higher (your number is higher than this guess), lower (your number is lower than this guess), correct, or exit.\n",
    "    Enter higher/lower/correct/exit: higher\n",
    "    HITL> The agent guessed: 10. Type one of: higher (your number is higher than this guess), lower (your number is lower than this guess), correct, or exit.\n",
    "    Enter higher/lower/correct/exit: lower\n",
    "    HITL> The agent guessed: 9. Type one of: higher (your number is higher than this guess), lower (your number is lower than this guess), correct, or exit.\n",
    "    Enter higher/lower/correct/exit: correct\n",
    "    Workflow output: Guessed correctly: 9\n",
    "    \"\"\"  # noqa: E501\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     asyncio.run(main())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9da76f4",
   "metadata": {},
   "source": [
    "### Step 5: Try it yourself\n",
    "Use the helper below. In notebooks, it detects the active event loop and calls `await main()`. In a plain Python process it falls back to `asyncio.run(main())`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "231ae0f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State: IN_PROGRESS_PENDING_REQUESTS (requests outstanding)\n",
      "State: IDLE_WITH_PENDING_REQUESTS (awaiting human input)\n",
      "HITL> The agent guessed: 5. Type one of: higher (your number is higher than this guess), lower (your number is lower than this guess), correct, or exit.\n",
      "State: IN_PROGRESS_PENDING_REQUESTS (requests outstanding)\n",
      "State: IDLE_WITH_PENDING_REQUESTS (awaiting human input)\n",
      "HITL> The agent guessed: 5. Type one of: higher (your number is higher than this guess), lower (your number is lower than this guess), correct, or exit.\n",
      "State: IN_PROGRESS_PENDING_REQUESTS (requests outstanding)\n",
      "State: IDLE_WITH_PENDING_REQUESTS (awaiting human input)\n",
      "HITL> The agent guessed: 3. Type one of: higher (your number is higher than this guess), lower (your number is lower than this guess), correct, or exit.\n",
      "State: IN_PROGRESS_PENDING_REQUESTS (requests outstanding)\n",
      "State: IDLE_WITH_PENDING_REQUESTS (awaiting human input)\n",
      "HITL> The agent guessed: 3. Type one of: higher (your number is higher than this guess), lower (your number is lower than this guess), correct, or exit.\n",
      "State: IN_PROGRESS_PENDING_REQUESTS (requests outstanding)\n",
      "State: IDLE_WITH_PENDING_REQUESTS (awaiting human input)\n",
      "HITL> The agent guessed: 1. Type one of: higher (your number is higher than this guess), lower (your number is lower than this guess), correct, or exit.\n",
      "Workflow output: Guessed correctly: 1\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "\n",
    "# Helper for notebooks vs. scripts\n",
    "loop = asyncio.get_event_loop()\n",
    "if loop.is_running():\n",
    "    # Jupyter/VS Code notebooks already have an event loop, so await directly.\n",
    "    await main()\n",
    "else:\n",
    "    asyncio.run(main())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17dbac56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "\n",
    "# Helper for notebooks vs. scripts\n",
    "loop = asyncio.get_event_loop()\n",
    "if loop.is_running():\n",
    "    # Jupyter/VS Code notebooks already have an event loop, so await directly.\n",
    "    await main()\n",
    "else:\n",
    "    asyncio.run(main())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "oai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
